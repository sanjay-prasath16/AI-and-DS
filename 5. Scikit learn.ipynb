{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "91a287a3-1ceb-4705-9532-375dcd19148a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# why scikit learn\n",
    "# building machine learning models (classification, regression, clustering)\n",
    "# performing data processing (scaling, encoding, splitting data)\n",
    "# evaluating models with metrics like accuracy, confusion matrix, cross-validation\n",
    "# pipeline building for ML workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e5790272-7a5a-4aca-9edc-4b447bf56a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "\n",
    "digits = load_digits()\n",
    "data = digits.images[30:70].reshape((40, -1))\n",
    "target = digits.target[30:70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8110e2c4-7fee-487b-9d78-a1b07a15331a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split - evaluate the generalization performance on unseen data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(data, target, test_size=0.2, shuffle=False)\n",
    "# if we give shuffle as false it will take last 20% value as for testing otherwise it will choose the training datas as randomly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0bbb9910-38e1-46db-8256-555b7fa17b00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.50%\n"
     ]
    }
   ],
   "source": [
    "# testing it with logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = LogisticRegression(max_iter=10000) \n",
    "model.fit(X_train, Y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(Y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "112871b1-8cfc-4f5d-9a55-cdc228b6f77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold Accuracies: [0.75  0.75  1.    1.    0.875]\n",
      "Average Accuracy: 0.875\n"
     ]
    }
   ],
   "source": [
    "# cross validation - repeating the split such that the training and testing sets are different for each evaluation\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import numpy as np\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=False)\n",
    "test_scores = cross_val_score(model, data, target, cv=cv)\n",
    "\n",
    "print(\"Fold Accuracies:\", test_scores)\n",
    "\n",
    "mean_accuracy = np.mean(test_scores)\n",
    "print(\"Average Accuracy:\", mean_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b4bf9f5b-387d-4ee0-a0e5-d84632dccc33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shuffle Accuracies: [1. 1.]\n"
     ]
    }
   ],
   "source": [
    "# other than KFold there are many ways to implement cross validation one of the example is shuffle split\n",
    "# ShuffleSplit - where the number of splits no longer determines the size of the train and test sets.\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "scv = ShuffleSplit(n_splits=2, test_size=0.2, random_state=0)\n",
    "stest_scores = cross_val_score(model, data, target, cv=scv)\n",
    "\n",
    "print(\"Shuffle Accuracies:\", stest_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bcb01396-9ad4-4da3-90c3-b86ebdf2ce96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: [0 1 2 0 2 1 1 1 1 2 1 0 0 2 1 2 0 1 2 1 0 0 2 2 0 1 2 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# visualizing scikit-learn pipelines - chains processing and modelings steps into one object to ensure clean code, prevent data leakage\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Step 1: Load the data\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Step 2: Split the data (half to train, half to test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Step 3: pipelining\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', LogisticRegression())\n",
    "])\n",
    "\n",
    "# Step 4: fitting the pipeline\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Step 5: make predictions\n",
    "predictions = pipeline.predict(X_test)\n",
    "\n",
    "print(\"Predictions:\", predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fdb9e38-eba5-42a6-83cc-fd5928ec815b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# under fitting - happens when the model is too simple to learn the data well\n",
    "    # Reason: Bias\n",
    "    # It means the model make strong assumption and cant capture the true patterns well. ( hight bias -> overfitting )\n",
    "    # to solve this issue - by using complex model, adding more features, reducing regularization, training more time.\n",
    "# over fitting - happens when the model learns the training data too well, including noise and fails to generalize to new data. \n",
    "    # Reason: Variance\n",
    "    # It means the model is too sensitive to the training data and learns even from the noise. ( high variance -> underfitting )\n",
    "    # to solve this issue - using simpler model, adding more training data, using regularization (l1, l2), dropping noisy features."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
